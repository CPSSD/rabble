{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./datasets/football_key.tsv\") as f:\n",
    "    # Each line is of form: <country_id> <country_name>\n",
    "    def fmt(line):\n",
    "        return (int(line[0])-1, line[1].strip('\"'))\n",
    "    data_key = [fmt(line.strip().split()) for line in f if line[0] != '*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./datasets/football_pairs.tsv\") as f:\n",
    "    # Each line is of form: <country_a_id> <country_b_id> <number_of_players>\n",
    "    def fmt(pair):\n",
    "        return (int(pair[0])-1, int(pair[1])-1, 1)\n",
    "    data_pairs = [fmt(line.strip().split()) for line in f if line[0] != '*']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the neighbours[] sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours = [set() for _ in range(len(data_key))]\n",
    "for p in data_pairs:\n",
    "    neighbours[p[0]].add(p[1])\n",
    "    neighbours[p[1]].add(p[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the similarity metric: graph distance.\n",
    "\n",
    "Method to compute graph distance without Dijkstra or similar taken from _Scalable Proximity Estimation and Link Prediction in Online Social Networks_ - Han Hee Song, Tae Won Cho, Vacha Dave, Yin Zhang, Lili Qiu:\n",
    "\n",
    "We initialize S = {x} and D = {y}. In each step we either expand set S to include its members’ neighbors (i.e., S = S ∪ {v|⟨u, v⟩ ∈ E ∧ u ∈ S}) or expand set D to include its members’ inverse neighbors (i.e., D = D ∪ {u|⟨u, v⟩ ∈ E ∧ v ∈ D}). We stop whenever S ∩ D != ∅ . The number of steps taken so far gives the shortest path distance. For efficiency, we always expand the smaller set between S and D in each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_GD(x, y, ignore_set=None):\n",
    "    MAX_DIST = 6\n",
    "    def expand(nset):\n",
    "        for n in set(nset):\n",
    "            for m in neighbours[n]:\n",
    "                if (ignore_set is not None and\n",
    "                   ((n, m) in ignore_set or (m, n) in ignore_set)):\n",
    "                    # We should calculate without this link,\n",
    "                    # as it is in the test set for this iter.\n",
    "                    continue\n",
    "                nset.add(m)\n",
    "\n",
    "    s = set([x])\n",
    "    d = set([y])\n",
    "    dist = 0\n",
    "    while len(s & d) == 0 and dist <= MAX_DIST:\n",
    "        dist += 1\n",
    "        if len(d) < len(s):\n",
    "            expand(d)\n",
    "        else:\n",
    "            expand(s)\n",
    "    return -dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the similarities across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarities(ignore_set=None):\n",
    "    # S_GD[x][y] contains the similarity of nodes x and y using the Graph Distance (GD) metric.\n",
    "    S_GD = [[0 for _ in range(len(data_key))] for _ in range(len(data_key))]\n",
    "    for i in range(len(data_key)-1):\n",
    "        for j in range(0, len(data_key)):\n",
    "            S_GD[i][j] = similarity_GD(i, j, ignore_set=ignore_set)\n",
    "    return S_GD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ARG AUT BEL BGR BRA CHE CHL CMR COL DEU DNK ESP FRA GBR GRE HRV IRN\n",
      "ARG   0, -2, -2, -2, -2, -3, -1, -2, -1, -2, -2, -1, -2, -2, -3, -2, -3\n",
      "AUT  -2,  0, -2, -2, -2, -2, -2, -1, -2, -1, -2, -1, -1, -1, -2, -1, -2\n",
      "BEL  -2, -2,  0, -2, -2, -2, -2, -2, -2, -1, -2, -2, -1, -2, -2, -2, -2\n",
      "BGR  -2, -2, -2,  0, -2, -3, -3, -2, -2, -1, -2, -1, -2, -3, -3, -2, -2\n",
      "BRA  -2, -2, -2, -2,  0, -3, -2, -2, -1, -2, -2, -1, -1, -2, -3, -2, -3\n",
      "CHE  -3, -2, -2, -3, -3,  0, -3, -3, -3, -2, -3, -2, -2, -2, -4, -3, -3\n",
      "CHL  -1, -2, -2, -3, -2, -3,  0, -2, -2, -2, -2, -2, -2, -2, -3, -2, -3\n",
      "CMR  -2, -1, -2, -2, -2, -3, -2,  0, -2, -1, -2, -1, -1, -2, -1, -2, -2\n",
      "COL  -1, -2, -2, -2, -1, -3, -2, -2,  0, -2, -2, -1, -2, -2, -3, -2, -3\n",
      "DEU  -2, -1, -1, -1, -2, -2, -2, -1, -2,  0, -1, -1, -1, -2, -2, -1, -1\n",
      "DNK  -2, -2, -2, -2, -2, -3, -2, -2, -2, -1,  0, -1, -2, -1, -3, -2, -2\n",
      "ESP  -1, -1, -2, -1, -1, -2, -2, -1, -1, -1, -1,  0, -2, -2, -2, -1, -2\n",
      "FRA  -2, -1, -1, -2, -1, -2, -2, -1, -2, -1, -2, -2,  0, -2, -2, -2, -2\n",
      "GBR  -2, -1, -2, -3, -2, -2, -2, -2, -2, -2, -1, -2, -2,  0, -2, -1, -3\n",
      "GRE  -3, -2, -2, -3, -3, -4, -3, -1, -3, -2, -3, -2, -2, -2,  0, -3, -3\n",
      "HRV  -2, -1, -2, -2, -2, -3, -2, -2, -2, -1, -2, -1, -2, -1, -3,  0, -2\n",
      "IRN  -3, -2, -2, -2, -3, -3, -3, -2, -3, -1, -2, -2, -2, -3, -3, -2,  0\n"
     ]
    }
   ],
   "source": [
    "# A quick eyeball check of a subset of the data.\n",
    "\n",
    "# Similarities with all links included.\n",
    "S_GD = compute_similarities()\n",
    "\n",
    "num_to_print = len(data_key)//2\n",
    "print(' '*4 + ' '.join(d[1] for d in data_key[:num_to_print]))\n",
    "print('\\n'.join(data_key[i][1] + ' ' + ','.join('{:>3}'.format(c) for c in S_GD[i][:num_to_print]) for i in range(num_to_print)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into 10 disjoint subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(e) 118\n",
      "len(e)//10 = 11\n",
      "10 subsets:\n",
      "[(24, 28), (11, 21), (24, 32), (12, 21), (1, 9), (17, 23), (0, 11), (0, 8), (1, 7), (7, 26), (4, 11), (5, 34)]\n",
      "[(13, 24), (13, 25), (10, 24), (12, 33), (13, 29), (12, 34), (13, 15), (3, 31), (13, 17), (7, 31), (11, 25), (4, 8)]\n",
      "[(17, 24), (9, 11), (9, 15), (12, 23), (5, 23), (12, 30), (9, 25), (9, 28), (10, 17), (9, 23), (7, 17), (1, 13)]\n",
      "[(11, 23), (7, 12), (31, 34), (2, 23), (23, 34), (3, 9), (11, 27), (10, 29), (12, 17), (9, 24), (9, 33), (19, 33)]\n",
      "[(17, 34), (9, 32), (3, 26), (1, 17), (11, 17), (23, 24), (13, 28), (7, 11), (23, 32), (17, 25), (21, 30), (15, 31)]\n",
      "[(11, 15), (1, 15), (22, 27), (14, 28), (11, 28), (4, 27), (17, 33), (10, 31), (9, 17), (1, 34), (4, 26), (13, 34)]\n",
      "[(7, 19), (12, 20), (11, 33), (0, 6), (9, 30), (2, 12), (6, 17), (13, 18), (24, 34), (7, 9), (7, 14), (2, 17)]\n",
      "[(19, 20), (9, 16), (4, 19), (17, 21), (9, 10), (13, 32), (9, 34), (0, 17), (12, 29), (8, 32), (2, 24), (15, 17)]\n",
      "[(3, 11), (10, 11), (23, 31), (13, 33), (8, 11), (9, 12), (21, 26), (4, 17), (14, 25), (1, 12), (10, 13), (4, 12)]\n",
      "[(2, 9), (6, 32), (2, 28), (8, 17), (9, 21), (11, 34), (11, 24), (28, 31), (25, 29), (1, 11)]\n"
     ]
    }
   ],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for it in range(0, len(l), n):\n",
    "        yield l[it:it + n]\n",
    "        \n",
    "e = []\n",
    "predict = []\n",
    "for i in range(len(data_key)):\n",
    "    for j in range(i+1, len(data_key)):\n",
    "        if i in neighbours[j]:\n",
    "            e.append((i, j))\n",
    "        else:\n",
    "            predict.append((i, j))\n",
    "            \n",
    "# e now contains all link pairs\n",
    "# predict contains all non-existing links from the original data\n",
    "# each pair is a tuple (a, b), where a < b\n",
    "\n",
    "# We now randomly shuffle this list\n",
    "import random\n",
    "random.shuffle(e)\n",
    "\n",
    "print('len(e)', len(e))\n",
    "print('len(e)//10 =', len(e)//10)\n",
    "\n",
    "# Create e_prime, a list of 10 partitions\n",
    "e_prime = (list(chunks(e, len(e)//10 + 1)))\n",
    "\n",
    "# TODO(iandioch): Figure out why the following line is necessary?\n",
    "# e_prime = e_prime[0]\n",
    "\n",
    "# The following is a quick eyeball test to make sure the partitions look ok.\n",
    "print('10 subsets:')\n",
    "for i in range(len(e_prime)):\n",
    "    entry = e_prime[i]\n",
    "    print(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tn1   \tn2   \tn3   \tAUC\n",
      "Fold 1 :\t2072 \t3361 \t291  \t0.655573\n",
      "Fold 2 :\t1794 \t3069 \t861  \t0.581499\n",
      "Fold 3 :\t2280 \t3444 \t0    \t0.699161\n",
      "Fold 4 :\t2086 \t3084 \t554  \t0.633823\n",
      "Fold 5 :\t1725 \t3120 \t879  \t0.573899\n",
      "Fold 6 :\t1636 \t2777 \t1311 \t0.528389\n",
      "Fold 7 :\t1745 \t2275 \t1704 \t0.503581\n",
      "Fold 8 :\t1674 \t2522 \t1528 \t0.512753\n",
      "Fold 9 :\t1803 \t3063 \t858  \t0.582547\n",
      "Fold 10:\t1496 \t2682 \t592  \t0.594759\n",
      "Average:\t1831 \t2940 \t858  \t0.586599\n"
     ]
    }
   ],
   "source": [
    "aucs = []\n",
    "n1s = []\n",
    "n2s = []\n",
    "n3s = []\n",
    "ns = []\n",
    "\n",
    "# Column headings.\n",
    "print('\\t\\tn1   \\tn2   \\tn3   \\tAUC')\n",
    "\n",
    "# Iterate across the 10 folds.\n",
    "for i in range(10):\n",
    "    test = e_prime[i]\n",
    "    S_CN = compute_similarities(ignore_set=test)\n",
    "    \n",
    "    n1 = 0 # missing_link > nonexistant_link\n",
    "    n2 = 0 # missing_link = nonexistant_link\n",
    "    n3 = 0 # missing_link < nonexistant_link\n",
    "    n = 0 # total link comparisons\n",
    "    for missing_link in test:\n",
    "        a_score = S_CN[missing_link[0]][missing_link[1]]\n",
    "        for nonexistant_link in predict:\n",
    "            b_score = S_CN[nonexistant_link[0]][nonexistant_link[1]]\n",
    "            if abs(a_score-b_score) < 0.0005:\n",
    "                n2 += 1\n",
    "            elif a_score > b_score:\n",
    "                n1 += 1\n",
    "            else:\n",
    "                n3 += 1\n",
    "            n += 1\n",
    "    auc = (n1 + 0.5*n2)/(n)\n",
    "    aucs.append(auc)\n",
    "    n1s.append(n1)\n",
    "    n2s.append(n2)\n",
    "    n3s.append(n3)\n",
    "    ns.append(n)\n",
    "    print('Fold {:<2}:\\t{:<5}\\t{:<5}\\t{:<5}\\t{:<.6f}'.format(i+1, n1, n2, n3, auc))\n",
    "\n",
    "def avg(seq):\n",
    "    return sum(seq)/len(seq)\n",
    "\n",
    "print('Average:\\t{:<5}\\t{:<5}\\t{:<5}\\t{:<.6f}'.format(int(round(avg(n1s))), int(round(avg(n2s))), int(round(avg(n3s))), avg(aucs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
